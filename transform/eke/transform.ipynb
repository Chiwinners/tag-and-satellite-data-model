{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d12593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Input:  C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\downloads\\eke\\sample\n",
      "🧺 Output Parquet (partitioned): C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\transform\\eke\\parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6831d38f2db41d29774faffcdb2809c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/30 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Parquet dataset written to: C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\transform\\eke\\parquet\n",
      "   Files processed: 30/30\n"
     ]
    }
   ],
   "source": [
    "# EKE (C3S DUACS L4) → lat, lon, timestamp, eke\n",
    "# Incremental write to Parquet partitioned by year/month\n",
    "#\n",
    "# - Handles disguised .nc that are actually ZIP/GZIP (no renaming; temp extraction).\n",
    "# - Prefers velocity anomalies (ugosa, vgosa). Fallback to absolute (ugos, vgos).\n",
    "# - Uses xarray+netCDF4/h5netcdf/scipy; writes Parquet with pyarrow.\n",
    "#\n",
    "# Requirements:\n",
    "#   pip install xarray netCDF4 h5netcdf h5py pyarrow tqdm pandas numpy\n",
    "\n",
    "from pathlib import Path\n",
    "import os, zipfile, gzip, tempfile, shutil, importlib\n",
    "from datetime import timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# --------------------------\n",
    "# Locate input/output\n",
    "# --------------------------\n",
    "def find_eke_sample_dir(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for parent in [start, *start.parents]:\n",
    "        cand = parent / \"downloads\" / \"eke\" / \"sample\"\n",
    "        if cand.is_dir():\n",
    "            return cand\n",
    "    raise FileNotFoundError(f\"Could not find 'downloads/eke/sample' starting from {start}\")\n",
    "\n",
    "SAMPLE_DIR = find_eke_sample_dir()\n",
    "REPO_ROOT  = SAMPLE_DIR.parents[2]  # .../downloads/eke/sample -> up 3 levels\n",
    "OUT_PARQ   = (REPO_ROOT / \"transform\" / \"eke\" / \"sample\")\n",
    "OUT_PARQ.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📂 Input:  {SAMPLE_DIR}\")\n",
    "print(f\"🧺 Output Parquet (partitioned): {OUT_PARQ}\")\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "PREFERRED_UV = [(\"ugosa\",\"vgosa\"), (\"ugos\",\"vgos\")]  # anomalies first\n",
    "LAT_CANDS = (\"latitude\",\"lat\")\n",
    "LON_CANDS = (\"longitude\",\"lon\")\n",
    "TIME_NAME = \"time\"\n",
    "\n",
    "FRACTION = 1.0                # 0<frac<=1 to randomly keep a fraction of points\n",
    "RANDOM_SEED = 42\n",
    "MAX_POINTS_PER_FILE = None    # cap rows per file (after FRACTION), e.g., 1_000_000\n",
    "CHUNK_ROWS = 2_000_000        # write in chunks to control memory\n",
    "COMPRESSION = \"snappy\"        # snappy|zstd|gzip\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def sniff_magic(path: Path) -> str:\n",
    "    with path.open(\"rb\") as f:\n",
    "        head = f.read(16)\n",
    "    if head.startswith(b\"\\x89HDF\\r\\n\\x1a\\n\"):\n",
    "        return \"HDF5\"      # NetCDF4/HDF5\n",
    "    if head.startswith(b\"CDF\"):\n",
    "        return \"NETCDF3\"   # NetCDF3\n",
    "    if head.startswith(b\"PK\"):\n",
    "        return \"ZIP\"\n",
    "    if head.startswith(b\"\\x1f\\x8b\"):\n",
    "        return \"GZIP\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def extract_temp_nc(src_path: Path, kind: str) -> tuple[Path, tempfile.TemporaryDirectory]:\n",
    "    \"\"\"Extract inner .nc from ZIP/GZIP into a TemporaryDirectory; return (nc_path, tmpdir).\"\"\"\n",
    "    tmpdir = tempfile.TemporaryDirectory()\n",
    "    out_nc = Path(tmpdir.name) / (src_path.stem + \"__real.nc\")\n",
    "    if kind == \"ZIP\":\n",
    "        with zipfile.ZipFile(src_path) as zf:\n",
    "            members = zf.namelist()\n",
    "            nc_members = [m for m in members if m.lower().endswith(\".nc\")]\n",
    "            pick = nc_members[0] if nc_members else members[0]\n",
    "            with zf.open(pick) as src, out_nc.open(\"wb\") as dst:\n",
    "                shutil.copyfileobj(src, dst)\n",
    "    elif kind == \"GZIP\":\n",
    "        with gzip.open(src_path, \"rb\") as src, out_nc.open(\"wb\") as dst:\n",
    "            shutil.copyfileobj(src, dst)\n",
    "    else:\n",
    "        tmpdir.cleanup()\n",
    "        raise ValueError(f\"Unsupported archive kind: {kind}\")\n",
    "    return out_nc, tmpdir\n",
    "\n",
    "def try_open_xr(nc_path: Path):\n",
    "    tried, last_err = [], None\n",
    "    for eng in (\"netcdf4\", \"h5netcdf\", \"scipy\"):\n",
    "        if eng == \"netcdf4\" and importlib.util.find_spec(\"netCDF4\") is None:\n",
    "            continue\n",
    "        if eng == \"h5netcdf\" and importlib.util.find_spec(\"h5netcdf\") is None:\n",
    "            continue\n",
    "        try:\n",
    "            ds = xr.open_dataset(nc_path, engine=eng, decode_cf=True, mask_and_scale=True)\n",
    "            return ds, eng\n",
    "        except Exception as e:\n",
    "            tried.append(eng); last_err = e\n",
    "    raise RuntimeError(f\"xarray failed with engines {tried}. Last error: {last_err}\")\n",
    "\n",
    "def choose_coords(ds: xr.Dataset):\n",
    "    lat_name = next((n for n in LAT_CANDS if n in ds.coords), None)\n",
    "    lon_name = next((n for n in LON_CANDS if n in ds.coords), None)\n",
    "    if lat_name is None or lon_name is None:\n",
    "        raise KeyError(\"latitude/longitude coords not found\")\n",
    "    lat = ds[lat_name].values\n",
    "    lon = ds[lon_name].values\n",
    "    return lat, lon, lat_name, lon_name\n",
    "\n",
    "def choose_uv(ds: xr.Dataset) -> tuple[str,str,str]:\n",
    "    for u, v in PREFERRED_UV:\n",
    "        if u in ds.data_vars and v in ds.data_vars:\n",
    "            return u, v, (\"anomaly\" if (u.endswith(\"osa\") and v.endswith(\"osa\")) else \"absolute\")\n",
    "    raise KeyError(\"No suitable velocity variables found (tried ugosa/vgosa and ugos/vgos)\")\n",
    "\n",
    "def write_parquet_block(df: pd.DataFrame):\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "    df[\"year\"] = df[\"ts\"].dt.year.astype(\"int16\")\n",
    "    df[\"month\"] = df[\"ts\"].dt.month.astype(\"int8\")\n",
    "    table = pa.Table.from_pandas(df[[\"lat\",\"lon\",\"timestamp\",\"eke\",\"year\",\"month\"]], preserve_index=False)\n",
    "    pq.write_to_dataset(table, root_path=OUT_PARQ, partition_cols=[\"year\",\"month\"], compression=COMPRESSION)\n",
    "\n",
    "# --------------------------\n",
    "# Processing\n",
    "# --------------------------\n",
    "files = sorted(SAMPLE_DIR.glob(\"*.nc\"))\n",
    "assert files, f\"No .nc files found in {SAMPLE_DIR}\"\n",
    "\n",
    "skipped, processed = [], 0\n",
    "\n",
    "for src in tqdm(files, desc=\"Files\", unit=\"file\"):\n",
    "    tmpdir = None\n",
    "    work = src\n",
    "    kind = sniff_magic(src)\n",
    "    if kind in (\"ZIP\",\"GZIP\"):\n",
    "        try:\n",
    "            work, tmpdir = extract_temp_nc(src, kind)\n",
    "        except Exception as e:\n",
    "            skipped.append((src.name, f\"extract {kind} failed: {e}\"))\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        ds, eng = try_open_xr(work)\n",
    "    except Exception as e:\n",
    "        skipped.append((src.name, f\"xarray open failed: {e}\"))\n",
    "        if tmpdir: \n",
    "            try: tmpdir.cleanup()\n",
    "            except: pass\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Coordinates\n",
    "        lat1d, lon1d, lat_name, lon_name = choose_coords(ds)\n",
    "        if TIME_NAME not in ds.coords:\n",
    "            raise KeyError(\"time coord not found\")\n",
    "        times = pd.to_datetime(ds[TIME_NAME].values).tz_localize(\"UTC\", nonexistent='shift_forward', ambiguous='NaT')\n",
    "\n",
    "        # Velocity fields\n",
    "        u_name, v_name, vel_kind = choose_uv(ds)\n",
    "        u = ds[u_name]  # (time, lat, lon)\n",
    "        v = ds[v_name]\n",
    "\n",
    "        # Shapes\n",
    "        # Expect (T, Y, X) or possibly named dims; enforce with .transpose if needed\n",
    "        def ensure_tyxd(a: xr.DataArray):\n",
    "            dims = list(a.dims)\n",
    "            want = [TIME_NAME, lat_name, lon_name]\n",
    "            if dims != want:\n",
    "                return a.transpose(*want)\n",
    "            return a\n",
    "        u = ensure_tyxd(u)\n",
    "        v = ensure_tyxd(v)\n",
    "\n",
    "        # Meshgrid for lat/lon (1D → 2D)\n",
    "        if lat1d.ndim == 1 and lon1d.ndim == 1:\n",
    "            lon2d, lat2d = np.meshgrid(lon1d, lat1d)   # (Y,X)\n",
    "        else:\n",
    "            # already 2D fields (rare)\n",
    "            lat2d, lon2d = lat1d, lon1d\n",
    "\n",
    "        # For each time slice (usually one per file)\n",
    "        for ti, tval in enumerate(times):\n",
    "            u2d = np.asarray(u.isel({TIME_NAME: ti}).values)\n",
    "            v2d = np.asarray(v.isel({TIME_NAME: ti}).values)\n",
    "\n",
    "            # EKE = 0.5*(u'^2 + v'^2); using anomalies if available\n",
    "            eke2d = 0.5*(u2d**2 + v2d**2)\n",
    "\n",
    "            # Flatten valid points\n",
    "            latf = lat2d.ravel()\n",
    "            lonf = lon2d.ravel()\n",
    "            ekef = eke2d.ravel()\n",
    "            finite = np.isfinite(latf) & np.isfinite(lonf) & np.isfinite(ekef)\n",
    "            latf, lonf, ekef = latf[finite], lonf[finite], ekef[finite]\n",
    "            if latf.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Optional downsample\n",
    "            n = latf.size\n",
    "            if FRACTION < 1.0:\n",
    "                k = max(1, int(np.ceil(n * FRACTION)))\n",
    "                idx = rng.choice(n, size=k, replace=False)\n",
    "                latf, lonf, ekef = latf[idx], lonf[idx], ekef[idx]\n",
    "                n = k\n",
    "\n",
    "            if (MAX_POINTS_PER_FILE is not None) and (n > MAX_POINTS_PER_FILE):\n",
    "                idx = rng.choice(n, size=MAX_POINTS_PER_FILE, replace=False)\n",
    "                latf, lonf, ekef = latf[idx], lonf[idx], ekef[idx]\n",
    "                n = MAX_POINTS_PER_FILE\n",
    "\n",
    "            # Chunked write\n",
    "            ts_iso = pd.to_datetime(tval).tz_convert(\"UTC\").isoformat()\n",
    "            if (CHUNK_ROWS is not None) and (n > CHUNK_ROWS):\n",
    "                for i0 in range(0, n, CHUNK_ROWS):\n",
    "                    i1 = min(i0 + CHUNK_ROWS, n)\n",
    "                    df = pd.DataFrame({\n",
    "                        \"lat\": latf[i0:i1],\n",
    "                        \"lon\": lonf[i0:i1],\n",
    "                        \"timestamp\": ts_iso,\n",
    "                        \"eke\": ekef[i0:i1],\n",
    "                    })\n",
    "                    write_parquet_block(df)\n",
    "            else:\n",
    "                df = pd.DataFrame({\n",
    "                    \"lat\": latf,\n",
    "                    \"lon\": lonf,\n",
    "                    \"timestamp\": ts_iso,\n",
    "                    \"eke\": ekef,\n",
    "                })\n",
    "                write_parquet_block(df)\n",
    "\n",
    "        processed += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        skipped.append((src.name, f\"process failed: {e}\"))\n",
    "\n",
    "    finally:\n",
    "        try: ds.close()\n",
    "        except: pass\n",
    "        if tmpdir:\n",
    "            try: tmpdir.cleanup()\n",
    "            except: pass\n",
    "\n",
    "print(f\"\\n✅ Parquet dataset written to: {OUT_PARQ}\")\n",
    "print(f\"   Files processed: {processed}/{len(files)}\")\n",
    "if skipped:\n",
    "    print(\"\\n⚠️ Skipped:\")\n",
    "    for name, reason in skipped:\n",
    "        print(f\"  - {name}: {reason}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
