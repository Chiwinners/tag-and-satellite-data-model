{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3bcdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Input:  C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\downloads\\depth\\sample\n",
      "üß∫ Output Parquet (partitioned): C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\transform\\depth\\parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbee9f7e8cd64ab787e456b03cb4195c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/1 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Parquet dataset written to: C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\transform\\depth\\parquet\n"
     ]
    }
   ],
   "source": [
    "# ICESat-2 ATL24 (.h5) -> lat, lon, timestamp, depth  (incremental Parquet, partitioned by year/month)\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# --------------------------\n",
    "# Locate input (downloads/depth/sample) and output (<repo_root>/transform/depth/parquet)\n",
    "# --------------------------\n",
    "def find_depth_sample_dir(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for parent in [start, *start.parents]:\n",
    "        cand = parent / \"downloads\" / \"depth\" / \"sample\"\n",
    "        if cand.is_dir():\n",
    "            return cand\n",
    "    raise FileNotFoundError(f\"Could not find 'downloads/depth/sample' starting from {start}\")\n",
    "\n",
    "SAMPLE_DIR = find_depth_sample_dir()\n",
    "REPO_ROOT  = SAMPLE_DIR.parents[2]  # .../downloads/depth/sample -> go up 3 levels\n",
    "OUT_PARQ   = (REPO_ROOT / \"transform\" / \"depth\" / \"sample\")\n",
    "OUT_PARQ.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Input:  {SAMPLE_DIR}\")\n",
    "print(f\"üß∫ Output Parquet (partitioned): {OUT_PARQ}\")\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "BEAMS = (\"gt1l\",\"gt1r\",\"gt2l\",\"gt2r\",\"gt3l\",\"gt3r\")\n",
    "LAT_NAME = \"lat_ph\"\n",
    "LON_NAME = \"lon_ph\"\n",
    "DELTA_NAME = \"delta_time\"\n",
    "\n",
    "# Choose which height to treat as \"depth\". We export depth = SIGN * height_var\n",
    "DEPTH_VAR_CANDIDATES = (\"surface_h\", \"ortho_h\", \"ellipse_h\")\n",
    "DEPTH_SIGN = -1.0  # if height is \"up\" (meters), depth below surface as negative height -> use -1\n",
    "\n",
    "# Optional quality filters (disabled by default)\n",
    "MIN_CONFIDENCE = None         # e.g., 0.5 to keep high confidence only (uses beam/<confidence>)\n",
    "FILTER_LOW_CONF_FLAG = False  # if True and beam/low_confidence_flag exists, keep only 0\n",
    "FILTER_SENSOR_FLAG = True     # if True and beam/sensor_depth_exceeded exists, keep only 0\n",
    "\n",
    "# Downsampling / memory control\n",
    "FRACTION = 1.0                # 0<frac<=1 to randomly keep a fraction of points\n",
    "RANDOM_SEED = 42\n",
    "MAX_POINTS_PER_BEAM = None    # e.g., 1_000_000 to cap per-beam rows\n",
    "CHUNK_ROWS = 2_000_000        # if a beam has > CHUNK_ROWS, write in chunks\n",
    "\n",
    "COMPRESSION = \"snappy\"        # snappy|zstd|gzip\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# --------------------------\n",
    "# Helpers\n",
    "# --------------------------\n",
    "def parse_utc(s: str | bytes | np.ndarray):\n",
    "    \"\"\"Parse ISO-like UTC string(s) such as '2019-02-12T03:04:05.123Z' into timezone-aware datetime.\"\"\"\n",
    "    if isinstance(s, (bytes, np.bytes_)):\n",
    "        s = s.decode(\"utf-8\", errors=\"ignore\")\n",
    "    if isinstance(s, np.ndarray) and s.shape == ():\n",
    "        s = s.item()\n",
    "        if isinstance(s, (bytes, np.bytes_)):\n",
    "            s = s.decode(\"utf-8\", errors=\"ignore\")\n",
    "    s = str(s).strip().replace(\"Z\", \"+00:00\")\n",
    "    return datetime.fromisoformat(s).astimezone(timezone.utc)\n",
    "\n",
    "def read_scalar(f, path):\n",
    "    \"\"\"Read scalar dataset if exists, else return None.\"\"\"\n",
    "    try:\n",
    "        dset = f[path]\n",
    "        val = dset[()]  # scalar\n",
    "        return val\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def choose_depth_var(f, beam):\n",
    "    \"\"\"Pick the first available height variable to map to depth.\"\"\"\n",
    "    for v in DEPTH_VAR_CANDIDATES:\n",
    "        p = f\"{beam}/{v}\"\n",
    "        if p in f:\n",
    "            return v\n",
    "    return None\n",
    "\n",
    "def write_parquet_block(df: pd.DataFrame):\n",
    "    \"\"\"Write a pandas DataFrame block to the Parquet dataset partitioned by year/month.\"\"\"\n",
    "    # Ensure proper dtypes\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "    df[\"year\"] = df[\"ts\"].dt.year.astype(\"int16\")\n",
    "    df[\"month\"] = df[\"ts\"].dt.month.astype(\"int8\")\n",
    "    table = pa.Table.from_pandas(df[[\"lat\",\"lon\",\"timestamp\",\"depth\",\"year\",\"month\"]], preserve_index=False)\n",
    "    pq.write_to_dataset(table, root_path=OUT_PARQ, partition_cols=[\"year\",\"month\"], compression=COMPRESSION)\n",
    "\n",
    "# --------------------------\n",
    "# Processing\n",
    "# --------------------------\n",
    "import h5py  # use h5py for robust HDF5 access\n",
    "\n",
    "files = sorted(list(SAMPLE_DIR.glob(\"*.h5\")) + list(SAMPLE_DIR.glob(\"*.hdf5\")))\n",
    "assert files, f\"No .h5/.hdf5 files found in {SAMPLE_DIR}\"\n",
    "\n",
    "skipped = []\n",
    "\n",
    "for path in tqdm(files, desc=\"Files\", unit=\"file\"):\n",
    "    try:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            # --- Granule timing anchors (robust per-point UTC via delta_time offset) ---\n",
    "            # We use: timestamp = granule_start_utc + (delta_time - start_delta_time) seconds\n",
    "            # This avoids knowing the absolute GPS epoch/leap seconds.\n",
    "            start_utc_raw = read_scalar(f, \"ancillary_data/granule_start_utc\") or read_scalar(f, \"ancillary_data/data_start_utc\")\n",
    "            end_utc_raw   = read_scalar(f, \"ancillary_data/granule_end_utc\")   or read_scalar(f, \"ancillary_data/data_end_utc\")\n",
    "            start_delta   = read_scalar(f, \"ancillary_data/start_delta_time\")\n",
    "\n",
    "            if start_utc_raw is None or start_delta is None:\n",
    "                skipped.append((path.name, \"missing ancillary_data/granule_start_utc or start_delta_time\"))\n",
    "                continue\n",
    "\n",
    "            t0 = parse_utc(start_utc_raw)  # timezone-aware datetime\n",
    "            t0_pd = pd.Timestamp(t0)       # pandas Timestamp with tz UTC\n",
    "\n",
    "            # --- Iterate over beams present in file ---\n",
    "            for beam in BEAMS:\n",
    "                if beam not in f:\n",
    "                    continue\n",
    "                g = f[beam]\n",
    "\n",
    "                # Required datasets\n",
    "                req = {}\n",
    "                for k in (LAT_NAME, LON_NAME, DELTA_NAME):\n",
    "                    if k in g:\n",
    "                        req[k] = g[k][...]\n",
    "                    else:\n",
    "                        req[k] = None\n",
    "\n",
    "                if req[LAT_NAME] is None or req[LON_NAME] is None or req[DELTA_NAME] is None:\n",
    "                    skipped.append((path.name, f\"{beam}: missing {LAT_NAME}/{LON_NAME}/{DELTA_NAME}\"))\n",
    "                    continue\n",
    "\n",
    "                # Depth source\n",
    "                depth_var = choose_depth_var(f, beam)\n",
    "                if depth_var is None:\n",
    "                    skipped.append((path.name, f\"{beam}: no depth-like var among {DEPTH_VAR_CANDIDATES}\"))\n",
    "                    continue\n",
    "                depth_arr = g[depth_var][...]\n",
    "\n",
    "                lat = np.asarray(req[LAT_NAME])\n",
    "                lon = np.asarray(req[LON_NAME])\n",
    "                dlt = np.asarray(req[DELTA_NAME])  # seconds since mission epoch\n",
    "                # Per-point UTC: start + (delta - start_delta) seconds\n",
    "                ts = t0_pd + pd.to_timedelta(dlt - float(start_delta), unit=\"s\")\n",
    "\n",
    "                # Optional quality masks\n",
    "                mask = np.isfinite(lat) & np.isfinite(lon) & np.isfinite(depth_arr)\n",
    "                if FILTER_SENSOR_FLAG and \"sensor_depth_exceeded\" in g:\n",
    "                    mask &= (np.asarray(g[\"sensor_depth_exceeded\"][...]) == 0)\n",
    "                if FILTER_LOW_CONF_FLAG and \"low_confidence_flag\" in g:\n",
    "                    mask &= (np.asarray(g[\"low_confidence_flag\"][...]) == 0)\n",
    "                if MIN_CONFIDENCE is not None and \"confidence\" in g:\n",
    "                    mask &= (np.asarray(g[\"confidence\"][...]) >= float(MIN_CONFIDENCE))\n",
    "\n",
    "                lat = lat[mask]; lon = lon[mask]; depth = depth_arr[mask]; ts = ts[mask]\n",
    "\n",
    "                # Random downsampling\n",
    "                n = lat.size\n",
    "                if n == 0:\n",
    "                    continue\n",
    "                if FRACTION < 1.0:\n",
    "                    k = max(1, int(np.ceil(n * FRACTION)))\n",
    "                    idx = rng.choice(n, size=k, replace=False)\n",
    "                    lat, lon, depth, ts = lat[idx], lon[idx], depth[idx], ts[idx]\n",
    "                    n = k\n",
    "\n",
    "                # Cap rows per beam\n",
    "                if (MAX_POINTS_PER_BEAM is not None) and (n > MAX_POINTS_PER_BEAM):\n",
    "                    idx = rng.choice(n, size=MAX_POINTS_PER_BEAM, replace=False)\n",
    "                    lat, lon, depth, ts = lat[idx], lon[idx], depth[idx], ts[idx]\n",
    "                    n = MAX_POINTS_PER_BEAM\n",
    "\n",
    "                # Sign convention -> depth (meters, positive-down if DEPTH_SIGN = -1 and heights are positive-up)\n",
    "                depth = DEPTH_SIGN * depth\n",
    "\n",
    "                # Chunked write\n",
    "                if (CHUNK_ROWS is not None) and (n > CHUNK_ROWS):\n",
    "                    for i0 in range(0, n, CHUNK_ROWS):\n",
    "                        i1 = min(i0 + CHUNK_ROWS, n)\n",
    "                        df = pd.DataFrame({\n",
    "                            \"lat\": lat[i0:i1],\n",
    "                            \"lon\": lon[i0:i1],\n",
    "                            \"timestamp\": ts[i0:i1],\n",
    "                            \"depth\": depth[i0:i1],\n",
    "                        })\n",
    "                        write_parquet_block(df)\n",
    "                else:\n",
    "                    df = pd.DataFrame({\n",
    "                        \"lat\": lat,\n",
    "                        \"lon\": lon,\n",
    "                        \"timestamp\": ts,\n",
    "                        \"depth\": depth,\n",
    "                    })\n",
    "                    write_parquet_block(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        skipped.append((path.name, f\"open/process failed: {e}\"))\n",
    "\n",
    "print(f\"\\n‚úÖ Parquet dataset written to: {OUT_PARQ}\")\n",
    "if skipped:\n",
    "    print(\"\\n‚ö†Ô∏è Skipped:\")\n",
    "    for name, reason in skipped:\n",
    "        print(f\"  - {name}: {reason}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
