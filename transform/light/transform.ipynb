{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "742f3eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Input:  C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\downloads\\light\\sample\n",
      "🧺 Output Parquet (partitioned): C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\transform\\light\\sample\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7224e2f73f5459d87a1e44cf92d736d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files:   0%|          | 0/14 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Parquet dataset written to: C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\transform\\light\\sample\n"
     ]
    }
   ],
   "source": [
    "# MODIS L3b PAR (.x.nc) -> lat, lon, timestamp, light\n",
    "# - Lee OBPG L3b dentro de /level-3_binned_data: BinList (compuesto), par (compuesto), BinIndex\n",
    "# - Calcula media por bin: mean = sum / weights (si weights > 0; si no, asume mean ya provista)\n",
    "# - Convierte bin_num -> (lat, lon) usando rejilla ISIN (NROWS=4320 ó 2160, deducido de attrs)\n",
    "# - timestamp: punto medio entre time_coverage_start y time_coverage_end\n",
    "# - Salida: Parquet incremental particionado por year/month\n",
    "#\n",
    "# Requiere: pip install h5py numpy pandas pyarrow tqdm\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import h5py\n",
    "\n",
    "# ==========================\n",
    "# Localización I/O\n",
    "# ==========================\n",
    "def find_light_sample_dir(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for parent in [start, *start.parents]:\n",
    "        cand = parent / \"downloads\" / \"light\" / \"sample\"\n",
    "        if cand.is_dir():\n",
    "            return cand\n",
    "    raise FileNotFoundError(f\"Could not find 'downloads/light/sample' starting from {start}\")\n",
    "\n",
    "SAMPLE_DIR = find_light_sample_dir()\n",
    "REPO_ROOT  = SAMPLE_DIR.parents[2]\n",
    "OUT_PARQ   = (REPO_ROOT / \"transform\" / \"light\" / \"sample\")\n",
    "OUT_PARQ.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📂 Input:  {SAMPLE_DIR}\")\n",
    "print(f\"🧺 Output Parquet (partitioned): {OUT_PARQ}\")\n",
    "\n",
    "# ==========================\n",
    "# Configuración\n",
    "# ==========================\n",
    "COMPRESSION = \"snappy\"\n",
    "CHUNK_ROWS = 2_000_000\n",
    "\n",
    "# (Opcional) Recorte por bounding box (SW/NE). Déjalo en None para no filtrar.\n",
    "# BBOX = (12.00284, -81.99937, 14.98947, -79.83031)\n",
    "BBOX = None\n",
    "\n",
    "# ==========================\n",
    "# Tiempo (corregido)\n",
    "# ==========================\n",
    "def parse_utc_iso8601(s: str) -> datetime:\n",
    "    s = (s or \"\").strip().replace(\"Z\", \"+00:00\")\n",
    "    return datetime.fromisoformat(s).astimezone(timezone.utc)\n",
    "\n",
    "def midpoint_time(attrs: dict) -> pd.Timestamp:\n",
    "    t0 = parse_utc_iso8601(attrs.get(\"time_coverage_start\", \"\"))\n",
    "    t1 = parse_utc_iso8601(attrs.get(\"time_coverage_end\", \"\"))\n",
    "    if isinstance(t0, datetime) and isinstance(t1, datetime):\n",
    "        return pd.Timestamp(t0 + (t1 - t0) / 2)  # no mezclar tz=\n",
    "    if isinstance(t0, datetime):\n",
    "        return pd.Timestamp(t0)\n",
    "    return pd.NaT\n",
    "\n",
    "def to_utc_series(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")  # sin utc=True si ya trae tz\n",
    "    if s.dt.tz is None:\n",
    "        return s.dt.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        return s.dt.tz_convert(\"UTC\")\n",
    "\n",
    "# ==========================\n",
    "# Grilla ISIN\n",
    "# ==========================\n",
    "def detect_nrows_from_attrs(attrs: dict) -> int:\n",
    "    #  ~9.27 km -> 2160; ~4.64 km -> 4320\n",
    "    res_km = None\n",
    "    for key in (\"geospatial_lat_resolution\", \"geospatial_lon_resolution\", \"spatialResolution\"):\n",
    "        v = attrs.get(key)\n",
    "        if v:\n",
    "            txt = str(v).lower().replace(\"kilometers\", \"km\").replace(\"kilometres\", \"km\")\n",
    "            for token in txt.split():\n",
    "                try:\n",
    "                    res_km = float(token)\n",
    "                    break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        if res_km:\n",
    "            break\n",
    "    if res_km is None:\n",
    "        return 4320\n",
    "    return 4320 if res_km < 7 else 2160\n",
    "\n",
    "_row_cache: dict[int, tuple[np.ndarray,np.ndarray,np.ndarray]] = {}\n",
    "\n",
    "def build_row_index(NROWS: int):\n",
    "    row_lat = np.empty(NROWS, dtype=np.float64)\n",
    "    nbin_row = np.empty(NROWS, dtype=np.int64)\n",
    "    basebin = np.empty(NROWS+1, dtype=np.int64)\n",
    "    basebin[0] = 1\n",
    "    for i in range(NROWS):\n",
    "        lat = 90.0 - 180.0 * (i + 0.5) / NROWS\n",
    "        row_lat[i] = lat\n",
    "        nbin = int(round(2 * NROWS * math.cos(math.radians(lat))))\n",
    "        if nbin < 1:\n",
    "            nbin = 1\n",
    "        nbin_row[i] = nbin\n",
    "        basebin[i+1] = basebin[i] + nbin\n",
    "    return row_lat, nbin_row, basebin\n",
    "\n",
    "def bin_to_latlon(bin_nums: np.ndarray, NROWS: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    if NROWS not in _row_cache:\n",
    "        _row_cache[NROWS] = build_row_index(NROWS)\n",
    "    row_lat, nbin_row, basebin = _row_cache[NROWS]\n",
    "    idx = np.searchsorted(basebin, bin_nums, side=\"right\") - 1\n",
    "    idx = np.clip(idx, 0, NROWS-1)\n",
    "    col = bin_nums - basebin[idx] - 1\n",
    "    lat = row_lat[idx]\n",
    "    lon = -180.0 + 360.0 * (col + 0.5) / nbin_row[idx]\n",
    "    return lat.astype(np.float32), lon.astype(np.float32)\n",
    "\n",
    "# ==========================\n",
    "# Escritura Parquet\n",
    "# ==========================\n",
    "def write_parquet_block(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "    df[\"ts\"] = to_utc_series(df[\"timestamp\"])\n",
    "    df[\"year\"] = df[\"ts\"].dt.year.astype(\"int16\")\n",
    "    df[\"month\"] = df[\"ts\"].dt.month.astype(\"int8\")\n",
    "    table = pa.Table.from_pandas(df[[\"lat\",\"lon\",\"timestamp\",\"light\",\"year\",\"month\"]], preserve_index=False)\n",
    "    pq.write_to_dataset(table, root_path=str(OUT_PARQ), partition_cols=[\"year\",\"month\"], compression=COMPRESSION)\n",
    "\n",
    "# ==========================\n",
    "# Utilidades HDF5\n",
    "# ==========================\n",
    "def attrs_collect(f: h5py.File) -> dict:\n",
    "    \"\"\"Toma attrs de raíz y, si faltan, intenta leer del grupo /processing_control/input_parameters.\"\"\"\n",
    "    out = {k: f.attrs[k] for k in f.attrs.keys()}\n",
    "    # Intenta completar algunos attrs frecuentes\n",
    "    try:\n",
    "        g = f[\"/processing_control/input_parameters\"]\n",
    "        for k in (\"geospatial_lat_resolution\", \"geospatial_lon_resolution\", \"spatialResolution\",\n",
    "                  \"time_coverage_start\", \"time_coverage_end\"):\n",
    "            if k not in out and k in g.attrs:\n",
    "                out[k] = g.attrs[k]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return {k: (v.decode() if isinstance(v, (bytes, np.bytes_)) else v) for k, v in out.items()}\n",
    "\n",
    "def get_group_level3(f: h5py.File) -> h5py.Group | None:\n",
    "    for key in (\"/level-3_binned_data\", \"/level-3 Binned Data\", \"/L3b\", \"/Level-3 Binned Data\"):\n",
    "        if key in f:\n",
    "            obj = f[key]\n",
    "            if isinstance(obj, h5py.Group):\n",
    "                return obj\n",
    "    # fallback: buscar por nombre aproximado\n",
    "    for name, obj in f.items():\n",
    "        if isinstance(obj, h5py.Group) and \"binned\" in name.lower():\n",
    "            return obj\n",
    "    return None\n",
    "\n",
    "# ==========================\n",
    "# Procesamiento principal\n",
    "# ==========================\n",
    "files = sorted(SAMPLE_DIR.glob(\"*.nc\"))\n",
    "assert files, f\"No .nc files found in {SAMPLE_DIR}\"\n",
    "\n",
    "skipped = []\n",
    "\n",
    "for path in tqdm(files, desc=\"Files\", unit=\"file\"):\n",
    "    try:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            attrs = attrs_collect(f)\n",
    "            ts_mid = midpoint_time(attrs)\n",
    "            if not isinstance(ts_mid, pd.Timestamp) or pd.isna(ts_mid):\n",
    "                skipped.append((path.name, \"missing/invalid time_coverage_*\"))\n",
    "                continue\n",
    "\n",
    "            g = get_group_level3(f)\n",
    "            if g is None:\n",
    "                skipped.append((path.name, \"group '/level-3_binned_data' not found\"))\n",
    "                continue\n",
    "\n",
    "            # Variables esperadas\n",
    "            if \"BinList\" not in g or \"par\" not in g:\n",
    "                skipped.append((path.name, \"BinList/par not found in level-3_binned_data\"))\n",
    "                continue\n",
    "\n",
    "            binlist = g[\"BinList\"]\n",
    "            par     = g[\"par\"]\n",
    "\n",
    "            # Lectura de campos compuestos\n",
    "            # BinList: ('bin_num','nobs','nscenes','weights','time_rec')\n",
    "            if binlist.dtype.names is None or par.dtype.names is None or \"sum\" not in par.dtype.names:\n",
    "                skipped.append((path.name, \"Unexpected compound dtypes in BinList/par\"))\n",
    "                continue\n",
    "\n",
    "            bin_num = np.array(binlist[\"bin_num\"][:], dtype=np.int64)\n",
    "            # preferimos weights si existe; si no, nobs como aproximación\n",
    "            if \"weights\" in binlist.dtype.names:\n",
    "                weights = np.array(binlist[\"weights\"][:], dtype=np.float64)\n",
    "            elif \"nobs\" in binlist.dtype.names:\n",
    "                weights = np.array(binlist[\"nobs\"][:], dtype=np.float64)\n",
    "            else:\n",
    "                weights = np.ones(bin_num.shape[0], dtype=np.float64)\n",
    "\n",
    "            par_sum = np.array(par[\"sum\"][:], dtype=np.float64)\n",
    "            if par_sum.shape[0] != bin_num.shape[0] or weights.shape[0] != bin_num.shape[0]:\n",
    "                skipped.append((path.name, f\"shape mismatch: par={par_sum.shape} weights={weights.shape} bin_num={bin_num.shape}\"))\n",
    "                continue\n",
    "\n",
    "            # Media por bin\n",
    "            with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "                mean_par = par_sum / np.where(weights > 0, weights, np.nan)\n",
    "            valid = np.isfinite(mean_par)\n",
    "            if not np.any(valid):\n",
    "                skipped.append((path.name, \"no valid PAR values after mean computation\"))\n",
    "                continue\n",
    "\n",
    "            bin_sel = bin_num[valid]\n",
    "            par_sel = mean_par[valid].astype(np.float32)\n",
    "\n",
    "            # Coordenadas (ISIN)\n",
    "            # Detecta NROWS (4320 típico 4.6km)\n",
    "            NROWS = detect_nrows_from_attrs(attrs)\n",
    "            lat, lon = bin_to_latlon(bin_sel, NROWS)\n",
    "\n",
    "            if BBOX is not None:\n",
    "                s, w, n, e = BBOX\n",
    "                m = (lat >= s) & (lat <= n) & (lon >= w) & (lon <= e)\n",
    "                if not np.any(m):\n",
    "                    skipped.append((path.name, \"no rows after BBOX filter\"))\n",
    "                    continue\n",
    "                lat, lon, par_sel = lat[m], lon[m], par_sel[m]\n",
    "\n",
    "            nrows = lat.size\n",
    "            ts_arr = np.repeat(pd.Timestamp(ts_mid), nrows)\n",
    "            df = pd.DataFrame({\n",
    "                \"lat\": lat,\n",
    "                \"lon\": lon,\n",
    "                \"timestamp\": ts_arr,   # tz-aware; to_utc_series maneja la tz\n",
    "                \"light\": par_sel,      # einstein m^-2 day^-1\n",
    "            })\n",
    "\n",
    "            # Escritura\n",
    "            if CHUNK_ROWS and nrows > CHUNK_ROWS:\n",
    "                for i0 in range(0, nrows, CHUNK_ROWS):\n",
    "                    i1 = min(i0 + CHUNK_ROWS, nrows)\n",
    "                    write_parquet_block(df.iloc[i0:i1])\n",
    "            else:\n",
    "                write_parquet_block(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        skipped.append((path.name, f\"open/process failed: {e}\"))\n",
    "\n",
    "print(f\"\\n✅ Parquet dataset written to: {OUT_PARQ}\")\n",
    "if skipped:\n",
    "    print(\"\\n⚠️ Skipped:\")\n",
    "    for name, reason in skipped:\n",
    "        print(f\"  - {name}: {reason}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
