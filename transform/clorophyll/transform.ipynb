{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "708cd821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Origen:  C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\downloads\\clorophyll\\sample\n",
      "üß∫ Parquet: C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\transform\\clorophyll\\sample (partitioned by year/month)\n",
      "üîå Engine:  netcdf4\n",
      "üóÇÔ∏è Files:   17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8fa0efb41846a9a30c40b92dac9c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/17 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Parquet dataset listo en: C:\\Users\\Crist\\Desktop\\NASA\\tag-and-satellite-data-model\\transform\\clorophyll\\sample\n"
     ]
    }
   ],
   "source": [
    "# MODIS L2 OC (group-aware) ‚Üí lat, lon, timestamp, chl_level\n",
    "# Escritura incremental a Parquet particionado (year/month)\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# --------------------------\n",
    "# Ubicar carpetas (origen/destino)\n",
    "# --------------------------\n",
    "def find_clorophyll_sample_dir(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for parent in [start, *start.parents]:\n",
    "        cand = parent / \"downloads\" / \"clorophyll\" / \"sample\"\n",
    "        if cand.is_dir(): return cand\n",
    "    raise FileNotFoundError(f\"Could not find 'downloads/clorophyll/sample' from {start}.\")\n",
    "\n",
    "SAMPLE_DIR = find_clorophyll_sample_dir()\n",
    "REPO_ROOT  = SAMPLE_DIR.parents[2]  # .../downloads/clorophyll/sample -> subir 3 niveles\n",
    "OUT_PARQ   = (REPO_ROOT / \"transform\" / \"clorophyll\" / \"sample\")\n",
    "OUT_PARQ.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "FRACTION = 1.0\n",
    "RANDOM_SEED = 42\n",
    "MAX_POINTS_PER_FILE = None  # e.g., 200_000\n",
    "COMPRESSION = \"snappy\"      # snappy|zstd|gzip\n",
    "\n",
    "# --------------------------\n",
    "# Utilidades\n",
    "# --------------------------\n",
    "def pick_engine():\n",
    "    if importlib.util.find_spec(\"netCDF4\") is None:\n",
    "        raise RuntimeError(\"Install netCDF4:  python -m pip install netCDF4\")\n",
    "    return \"netcdf4\"\n",
    "\n",
    "def midpoint_iso(t0, t1):\n",
    "    def parse_iso(s):\n",
    "        if not s: return None\n",
    "        s = str(s).strip().replace(\"Z\",\"+00:00\")\n",
    "        try: return datetime.fromisoformat(s).astimezone(timezone.utc)\n",
    "        except Exception: return None\n",
    "    a, b = parse_iso(t0), parse_iso(t1)\n",
    "    if a and b: return (a + (b - a)/2).isoformat().replace(\"+00:00\",\"Z\")\n",
    "    if a: return a.isoformat().replace(\"+00:00\",\"Z\")\n",
    "    if b: return b.isoformat().replace(\"+00:00\",\"Z\")\n",
    "    return None\n",
    "\n",
    "def timestamp_from_name(fname):\n",
    "    m = re.search(r\"(\\d{8}T\\d{6})\", fname)\n",
    "    if not m: return None\n",
    "    s = m.group(1)\n",
    "    return f\"{s[:4]}-{s[4:6]}-{s[6:8]}T{s[9:11]}:{s[11:13]}:{s[13:]}Z\"\n",
    "\n",
    "def list_variable_paths(nc_path: Path):\n",
    "    import netCDF4 as nc\n",
    "    var_paths = []\n",
    "    with nc.Dataset(nc_path, \"r\") as ds:\n",
    "        def walk(group, prefix=\"\"):\n",
    "            for vname in group.variables.keys():\n",
    "                var_paths.append(f\"{prefix}{vname}\" if prefix else vname)\n",
    "            for gname, subg in group.groups.items():\n",
    "                walk(subg, f\"{prefix}{gname}/\" if prefix else f\"{gname}/\")\n",
    "        walk(ds)\n",
    "    return var_paths\n",
    "\n",
    "def split_group_var(path_str: str):\n",
    "    if \"/\" not in path_str: return None, path_str\n",
    "    parts = path_str.split(\"/\")\n",
    "    return \"/\".join(parts[:-1]), parts[-1]\n",
    "\n",
    "def choose_chl_path(var_paths):\n",
    "    preferred = {\"chlor_a\",\"chlorophyll_a\",\"chlorophyll\"}\n",
    "    for p in var_paths:\n",
    "        if p.split(\"/\")[-1] in preferred: return p\n",
    "    for p in var_paths:\n",
    "        base = p.split(\"/\")[-1].lower()\n",
    "        if \"chlor\" in base or re.search(r\"\\bchl\\b\", base): return p\n",
    "    return None\n",
    "\n",
    "def choose_lat_lon_paths(var_paths, chl_shape, open_group_func):\n",
    "    lat_names = {\"latitude\",\"lat\"}\n",
    "    lon_names = {\"longitude\",\"lon\"}\n",
    "\n",
    "    # 2D exact match\n",
    "    for p in var_paths:\n",
    "        if p.split(\"/\")[-1].lower() in lat_names:\n",
    "            g_lat, v_lat = split_group_var(p)\n",
    "            try: arr_lat = np.asarray(open_group_func(g_lat)[v_lat].values)\n",
    "            except Exception: continue\n",
    "            if arr_lat.shape == chl_shape:\n",
    "                for q in var_paths:\n",
    "                    if q.split(\"/\")[-1].lower() in lon_names:\n",
    "                        g_lon, v_lon = split_group_var(q)\n",
    "                        try: arr_lon = np.asarray(open_group_func(g_lon)[v_lon].values)\n",
    "                        except Exception: continue\n",
    "                        if arr_lon.shape == chl_shape: return p, q, \"2D\"\n",
    "\n",
    "    # 1D meshgrid\n",
    "    if len(chl_shape) == 2:\n",
    "        ny, nx = chl_shape\n",
    "        lat1d, lon1d = [], []\n",
    "        for p in var_paths:\n",
    "            b = p.split(\"/\")[-1].lower()\n",
    "            if b in lat_names:\n",
    "                g,v = split_group_var(p)\n",
    "                try: arr = np.asarray(open_group_func(g)[v].values)\n",
    "                except Exception: continue\n",
    "                if arr.ndim == 1 and arr.size == ny: lat1d.append(p)\n",
    "            if b in lon_names:\n",
    "                g,v = split_group_var(p)\n",
    "                try: arr = np.asarray(open_group_func(g)[v].values)\n",
    "                except Exception: continue\n",
    "                if arr.ndim == 1 and arr.size == nx: lon1d.append(p)\n",
    "        if lat1d and lon1d: return lat1d[0], lon1d[0], \"1D\"\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "def flatten_points(lat_arr, lon_arr, chl_arr):\n",
    "    lat_f, lon_f, chl_f = lat_arr.ravel(), lon_arr.ravel(), chl_arr.ravel()\n",
    "    finite = np.isfinite(lat_f) & np.isfinite(lon_f) & np.isfinite(chl_f)\n",
    "    return lat_f[finite], lon_f[finite], chl_f[finite]\n",
    "\n",
    "# --------------------------\n",
    "# Proceso (escritura incremental)\n",
    "# --------------------------\n",
    "nc_files = sorted(SAMPLE_DIR.glob(\"*.nc\"))\n",
    "assert nc_files, f\"No .nc files found in {SAMPLE_DIR}\"\n",
    "engine = pick_engine()\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "skipped = []\n",
    "print(f\"üìÇ Origen:  {SAMPLE_DIR}\")\n",
    "print(f\"üß∫ Parquet: {OUT_PARQ} (partitioned by year/month)\")\n",
    "print(f\"üîå Engine:  {engine}\")\n",
    "print(f\"üóÇÔ∏è Files:   {len(nc_files)}\")\n",
    "\n",
    "for path in tqdm(nc_files, desc=\"Processing\", unit=\"file\"):\n",
    "    # 1) Descubrir variables en todos los grupos\n",
    "    try:\n",
    "        var_paths = list_variable_paths(path)\n",
    "    except Exception as e:\n",
    "        skipped.append((path.name, f\"list vars failed: {e}\"))\n",
    "        continue\n",
    "\n",
    "    def open_group(group_name):\n",
    "        if group_name in (None, \"\", \"/\"):\n",
    "            return xr.open_dataset(path, engine=engine, decode_cf=True, mask_and_scale=True)\n",
    "        return xr.open_dataset(path, engine=engine, decode_cf=True, mask_and_scale=True, group=group_name)\n",
    "\n",
    "    # 2) Timestamp del archivo\n",
    "    try:\n",
    "        root_ds = open_group(None)\n",
    "        ts_iso = midpoint_iso(root_ds.attrs.get(\"time_coverage_start\"),\n",
    "                              root_ds.attrs.get(\"time_coverage_end\")) \\\n",
    "                 or timestamp_from_name(path.name)\n",
    "    except Exception:\n",
    "        ts_iso = timestamp_from_name(path.name)\n",
    "    if ts_iso is None:\n",
    "        skipped.append((path.name, \"no timestamp available\"))\n",
    "        try: root_ds.close()\n",
    "        except: pass\n",
    "        continue\n",
    "\n",
    "    # 3) Clorofila\n",
    "    chl_path = choose_chl_path(var_paths)\n",
    "    if chl_path is None:\n",
    "        skipped.append((path.name, \"no chlorophyll-like var\"))\n",
    "        try: root_ds.close()\n",
    "        except: pass\n",
    "        continue\n",
    "\n",
    "    g_chl, v_chl = split_group_var(chl_path)\n",
    "    try:\n",
    "        ds_chl = open_group(g_chl)\n",
    "        chl_arr = np.asarray(ds_chl[v_chl].values)\n",
    "        chl_shape = chl_arr.shape\n",
    "    except Exception as e:\n",
    "        skipped.append((path.name, f\"read chl failed ({chl_path}): {e}\"))\n",
    "        for dso in (\"ds_chl\",\"root_ds\"):\n",
    "            try: locals().get(dso).close()\n",
    "            except: pass\n",
    "        continue\n",
    "\n",
    "    # 4) Geo\n",
    "    lat_path, lon_path, geo_kind = choose_lat_lon_paths(var_paths, chl_shape, open_group)\n",
    "    if lat_path is None or lon_path is None:\n",
    "        skipped.append((path.name, \"no lat/lon compatible\"))\n",
    "        for dso in (\"ds_chl\",\"root_ds\"):\n",
    "            try: locals().get(dso).close()\n",
    "            except: pass\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        g_lat, v_lat = split_group_var(lat_path)\n",
    "        g_lon, v_lon = split_group_var(lon_path)\n",
    "        ds_lat = open_group(g_lat); ds_lon = open_group(g_lon)\n",
    "        lat_val = np.asarray(ds_lat[v_lat].values)\n",
    "        lon_val = np.asarray(ds_lon[v_lon].values)\n",
    "        if geo_kind == \"1D\":\n",
    "            ny, nx = chl_arr.shape[-2], chl_arr.shape[-1]\n",
    "            lon_val, lat_val = np.meshgrid(lon_val, lat_val)\n",
    "    except Exception as e:\n",
    "        skipped.append((path.name, f\"read lat/lon failed: {e}\"))\n",
    "        for dso in (\"ds_chl\",\"ds_lat\",\"ds_lon\",\"root_ds\"):\n",
    "            try: locals().get(dso).close()\n",
    "            except: pass\n",
    "        continue\n",
    "\n",
    "    for dso in (ds_chl, ds_lat, ds_lon, root_ds):\n",
    "        try: dso.close()\n",
    "        except: pass\n",
    "\n",
    "    # 5) Aplanar, filtrar, submuestrear\n",
    "    lat_f, lon_f, chl_f = flatten_points(lat_val, lon_val, chl_arr)\n",
    "    if lat_f.size == 0:\n",
    "        skipped.append((path.name, \"no finite lat/lon/chl\"))\n",
    "        continue\n",
    "\n",
    "    if FRACTION < 1.0:\n",
    "        k = int(np.ceil(lat_f.size * FRACTION))\n",
    "        idx = rng.choice(lat_f.size, size=k, replace=False)\n",
    "        lat_f, lon_f, chl_f = lat_f[idx], lon_f[idx], chl_f[idx]\n",
    "\n",
    "    if (MAX_POINTS_PER_FILE is not None) and (lat_f.size > MAX_POINTS_PER_FILE):\n",
    "        idx = rng.choice(lat_f.size, size=MAX_POINTS_PER_FILE, replace=False)\n",
    "        lat_f, lon_f, chl_f = lat_f[idx], lon_f[idx], chl_f[idx]\n",
    "\n",
    "    # 6) ==== ESCRITURA INCREMENTAL A PARQUET ====\n",
    "    block = pd.DataFrame({\n",
    "        \"lat\": lat_f,\n",
    "        \"lon\": lon_f,\n",
    "        \"timestamp\": ts_iso,\n",
    "        \"chl_level\": chl_f,\n",
    "    })\n",
    "    block[\"ts\"] = pd.to_datetime(block[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "    block[\"year\"] = block[\"ts\"].dt.year.astype(\"int16\")\n",
    "    block[\"month\"] = block[\"ts\"].dt.month.astype(\"int8\")\n",
    "\n",
    "    table = pa.Table.from_pandas(block[[\"lat\",\"lon\",\"timestamp\",\"chl_level\",\"year\",\"month\"]],\n",
    "                                 preserve_index=False)\n",
    "    pq.write_to_dataset(table,\n",
    "                        root_path=OUT_PARQ,\n",
    "                        partition_cols=[\"year\",\"month\"],\n",
    "                        compression=COMPRESSION)\n",
    "\n",
    "# Reporte\n",
    "print(f\"\\n‚úÖ Parquet dataset listo en: {OUT_PARQ}\")\n",
    "if skipped:\n",
    "    print(\"\\n‚ö†Ô∏è Skipped files:\")\n",
    "    for name, reason in skipped:\n",
    "        print(f\"  - {name}: {reason}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c20c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
