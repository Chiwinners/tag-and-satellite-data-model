{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SST (lat, lon, timestamp, sst) -> gradiente de SST\n",
    "# Salida: <repo_root>/transform/sst/gradient  (Parquet particionado por year/month)\n",
    "\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as pads\n",
    "\n",
    "# =========================\n",
    "# Localizaci√≥n carpetas\n",
    "# =========================\n",
    "def find_sst_parquet_dir(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    # ruta donde guardaste el parquet de SST\n",
    "    for parent in [start, *start.parents]:\n",
    "        cand = parent / \"transform\" / \"sst\" / \"sample\"\n",
    "        if cand.is_dir():\n",
    "            return cand\n",
    "    raise FileNotFoundError(f\"No se encontr√≥ 'transform/sst/parquet' desde {start}\")\n",
    "\n",
    "IN_PARQ   = find_sst_parquet_dir()\n",
    "REPO_ROOT = IN_PARQ.parents[2]  # .../transform/sst/parquet -> subir 3 niveles\n",
    "OUT_PARQ  = (REPO_ROOT / \"transform\" / \"sst\" / \"gradient\")\n",
    "OUT_PARQ.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üì• Input : {IN_PARQ}\")\n",
    "print(f\"üì§ Output: {OUT_PARQ} (partitioned by year/month)\")\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "# KNN/Plano local: sst ‚âà a*x + b*y + c  => grad = (a, b) [unidades: sst por km]\n",
    "K_NEIGHBORS = 8         # vecinos por punto (>=3)\n",
    "MAX_RADIUS_KM = None    # opcional: descartar gradientes cuyo vecino m√°s lejano supere este radio (None = desactivado)\n",
    "FRACTION = 1.0          # submuestreo aleatorio por timestamp (0<frac<=1)\n",
    "RANDOM_SEED = 42\n",
    "CHUNK_SOLVE = 50_000    # resuelve gradientes por lotes para no agotar RAM\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# =========================\n",
    "# Utilidades parquet\n",
    "# =========================\n",
    "def write_parquet_block(df: pd.DataFrame):\n",
    "    \"\"\"Escribe bloque al dataset particionado por year/month.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "    df[\"year\"] = df[\"ts\"].dt.year.astype(\"int16\")\n",
    "    df[\"month\"] = df[\"ts\"].dt.month.astype(\"int8\")\n",
    "    table = pa.Table.from_pandas(\n",
    "        df[[\"lat\",\"lon\",\"timestamp\",\"sst\",\"dTdx_km\",\"dTdy_km\",\"sst_grad\",\"year\",\"month\"]],\n",
    "        preserve_index=False\n",
    "    )\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=OUT_PARQ,\n",
    "        partition_cols=[\"year\",\"month\"],\n",
    "        compression=\"snappy\"\n",
    "    )\n",
    "\n",
    "# =========================\n",
    "# Geom helpers\n",
    "# =========================\n",
    "def lonlat_to_local_km(lon: np.ndarray, lat: np.ndarray):\n",
    "    \"\"\"\n",
    "    Proyecci√≥n equirectangular local (aprox) -> (x,y) en km\n",
    "    Centro en (lon0, lat0) = medianas del grupo.\n",
    "    \"\"\"\n",
    "    lon0 = np.nanmedian(lon); lat0 = np.nanmedian(lat)\n",
    "    R = 6371.0  # km\n",
    "    to_rad = np.pi / 180.0\n",
    "    x = R * np.cos(lat0 * to_rad) * (lon - lon0) * to_rad\n",
    "    y = R * (lat - lat0) * to_rad\n",
    "    return x.astype(\"float32\"), y.astype(\"float32\")\n",
    "\n",
    "def fit_plane_gradients(x: np.ndarray, y: np.ndarray, z: np.ndarray,\n",
    "                        k: int, max_radius_km: float | None,\n",
    "                        chunk: int):\n",
    "    \"\"\"\n",
    "    Estima gradiente por punto usando KNN y ajuste de plano local (LS):\n",
    "      z ‚âà a*x + b*y + c  => grad = (a, b)\n",
    "    Devuelve dZ/dx (km^-1), dZ/dy (km^-1) y |grad|.\n",
    "    \"\"\"\n",
    "    from scipy.spatial import cKDTree\n",
    "\n",
    "    n = x.size\n",
    "    tree = cKDTree(np.c_[x, y])\n",
    "\n",
    "    dZdx = np.full(n, np.nan, dtype=\"float32\")\n",
    "    dZdy = np.full(n, np.nan, dtype=\"float32\")\n",
    "\n",
    "    # Resolve en lotes para controlar memoria/tiempo\n",
    "    for i0 in tqdm(range(0, n, chunk), desc=\"  -> resolviendo gradientes\", leave=False):\n",
    "        i1 = min(i0 + chunk, n)\n",
    "        pts = np.c_[x[i0:i1], y[i0:i1]]\n",
    "        # vecinos (incluye el propio punto)\n",
    "        dist, idx = tree.query(pts, k=min(k, n), workers=-1)\n",
    "\n",
    "        # asegurar 2D shape cuando k=1\n",
    "        if dist.ndim == 1:\n",
    "            dist = dist[:, None]; idx = idx[:, None]\n",
    "\n",
    "        for j in range(i1 - i0):\n",
    "            neigh = idx[j]\n",
    "            # opcional: filtrar por radio m√°ximo (si se defini√≥)\n",
    "            if max_radius_km is not None:\n",
    "                ok = dist[j] <= max_radius_km\n",
    "                # garantizar al menos 3 vecinos\n",
    "                if ok.sum() < 3:\n",
    "                    continue\n",
    "                neigh = neigh[ok]\n",
    "\n",
    "            if neigh.size < 3:\n",
    "                continue\n",
    "\n",
    "            xi = x[neigh].astype(\"float64\")\n",
    "            yi = y[neigh].astype(\"float64\")\n",
    "            zi = z[neigh].astype(\"float64\")\n",
    "\n",
    "            # centrar en el punto objetivo para robustez num√©rica\n",
    "            x0, y0 = pts[j]\n",
    "            Xi = xi - x0\n",
    "            Yi = yi - y0\n",
    "\n",
    "            A = np.c_[Xi, Yi, np.ones_like(Xi)]\n",
    "            try:\n",
    "                coef, *_ = np.linalg.lstsq(A, zi, rcond=None)  # [a, b, c]\n",
    "                dZdx[i0 + j] = coef[0]\n",
    "                dZdy[i0 + j] = coef[1]\n",
    "            except Exception:\n",
    "                # deja NaN si algo falla\n",
    "                pass\n",
    "\n",
    "    grad = np.sqrt(dZdx**2 + dZdy**2, dtype=\"float32\")\n",
    "    return dZdx, dZdy, grad\n",
    "\n",
    "# =========================\n",
    "# Pipeline\n",
    "# =========================\n",
    "ds = pads.dataset(IN_PARQ, format=\"parquet\", partitioning=\"hive\")\n",
    "print(\"üîé Escaneando timestamps √∫nicos...\")\n",
    "timestamps = set()\n",
    "\n",
    "# recolectar timestamps de forma streaming (columna sola)\n",
    "scan_ts = ds.scan(columns=[\"timestamp\"])\n",
    "for batch in tqdm(scan_ts.to_batches(), desc=\"  -> leyendo 'timestamp'\", leave=False):\n",
    "    col = batch.column(0)  # Arrow Array\n",
    "    # convertir a pylist es seguro aqu√≠ porque solo guardamos valores √∫nicos\n",
    "    for v in col.to_pylist():\n",
    "        if v is not None:\n",
    "            timestamps.add(v)\n",
    "\n",
    "timestamps = sorted(timestamps)\n",
    "print(f\"üïí Timestamps distintos: {len(timestamps)}\")\n",
    "\n",
    "# Proceso por timestamp (independiente, permite limpieza incremental)\n",
    "for ts in tqdm(timestamps, desc=\"Procesando timestamps\", unit=\"ts\"):\n",
    "    # lee solo filas de ese timestamp\n",
    "    filt = (pads.field(\"timestamp\") == pa.scalar(ts))\n",
    "    scanner = ds.scan(columns=[\"lat\",\"lon\",\"timestamp\",\"sst\"], filter=filt)\n",
    "\n",
    "    # Acumula en un √∫nico DataFrame por timestamp (normalmente est√° en 1-2 fragmentos)\n",
    "    parts = []\n",
    "    for batch in scanner.to_batches():\n",
    "        tbl = pa.Table.from_batches([batch])\n",
    "        parts.append(tbl.to_pandas())\n",
    "    if not parts:\n",
    "        continue\n",
    "    df = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    # limpiar\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"lat\",\"lon\",\"sst\"])\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # submuestreo opcional para acelerar\n",
    "    if FRACTION < 1.0 and len(df) > 0:\n",
    "        df = df.sample(frac=FRACTION, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    # coordenadas locales en km\n",
    "    x, y = lonlat_to_local_km(df[\"lon\"].to_numpy(), df[\"lat\"].to_numpy())\n",
    "    z = df[\"sst\"].to_numpy(dtype=\"float32\")\n",
    "\n",
    "    # gradiente local KNN\n",
    "    dZdx, dZdy, grad = fit_plane_gradients(\n",
    "        x, y, z,\n",
    "        k=K_NEIGHBORS,\n",
    "        max_radius_km=MAX_RADIUS_KM,\n",
    "        chunk=CHUNK_SOLVE\n",
    "    )\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"dTdx_km\"] = dZdx\n",
    "    out[\"dTdy_km\"] = dZdy\n",
    "    out[\"sst_grad\"] = grad  # magnitud en unidades de sst por km\n",
    "\n",
    "    # escribir (filtra filas sin gradiente v√°lido)\n",
    "    out = out.dropna(subset=[\"dTdx_km\",\"dTdy_km\",\"sst_grad\"])\n",
    "    if not out.empty:\n",
    "        write_parquet_block(out)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset de gradientes listo en: {OUT_PARQ}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
